#Program 1- AlexNet#
# -*- coding: utf-8 -*-
"""DL  modified codes

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N_FNFJENcAS2abniKsXPS8E7eZaCLhoz
"""

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2

class AlexNet(Sequential):
    def __init__(self, input_shape, num_classes):
        super().__init__()

        # First Convolutional Block
        self.add(Conv2D(96, (11, 11), strides=4, padding='valid', activation='relu',
                        kernel_regularizer=l2(0.0005), input_shape=input_shape))
        self.add(BatchNormalization())
        self.add(MaxPooling2D((3, 3), strides=2))

        # Second Convolutional Block
        self.add(Conv2D(256, (5, 5), padding='same', activation='relu', kernel_regularizer=l2(0.0005)))
        self.add(BatchNormalization())
        self.add(MaxPooling2D((3, 3), strides=2))

        # Third, Fourth, Fifth Convolutional Layers
        self.add(Conv2D(384, (3, 3), padding='same', activation='relu'))
        self.add(Conv2D(384, (3, 3), padding='same', activation='relu'))
        self.add(Conv2D(256, (3, 3), padding='same', activation='relu'))
        self.add(MaxPooling2D((3, 3), strides=2))

        # Flatten
        self.add(Flatten())

        # Fully Connected Layers (Modified)
        self.add(Dense(2048, activation='relu'))   # reduced from 4096 → 2048
        self.add(Dropout(0.5))
        self.add(Dense(1024, activation='relu'))   # reduced from 4096 → 1024
        self.add(Dropout(0.5))

        # Output Layer
        self.add(Dense(num_classes, activation='softmax'))

# Example usage
input_shape = (224, 224, 3)
num_classes = 10   # using fewer classes for a smaller dataset
model = AlexNet(input_shape, num_classes)
model.summary()



#program2-Q-learning on graph#
import numpy as np
import pylab as pl
import networkx as nx

# -----------------------------
# GRAPH DEFINITION
# -----------------------------
edges = [(0,1),(1,5),(5,6),(5,4),(1,2),
         (1,3),(9,10),(2,4),(0,6),(6,7),
         (8,9),(7,8),(1,7),(3,9)]

goal = 10
G = nx.Graph()
G.add_edges_from(edges)

pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True)
pl.show()

# -----------------------------
# REWARD MATRIX (UPDATED)
# -----------------------------
MATRIX_SIZE = 11
M = np.matrix(np.full((MATRIX_SIZE, MATRIX_SIZE), -1))

for (a,b) in edges:
    M[a,b] = 0
    M[b,a] = 0
    if b == goal:
        M[a,b] = 100
    if a == goal:
        M[b,a] = 100

M[goal,goal] = 100
print("Updated Reward Matrix:")
print(M)

# -----------------------------
# Q MATRIX
# -----------------------------
Q = np.zeros((MATRIX_SIZE, MATRIX_SIZE))

gamma = 0.85     # updated: encourages long-term planning
alpha = 0.6      # NEW: learning rate
epsilon = 0.2    # NEW: ε-greedy exploration
initial_state = 1


# -----------------------------
# FUNCTIONS
# -----------------------------
def available_actions(state):
    """Return non-negative reward actions."""
    return np.where(M[state] >= 0)[1]

def sample_next_action(actions):
    """Choose next action using proper sampling."""
    return np.random.choice(actions)

def epsilon_greedy(state):
    """NEW: Choose random action with probability ε."""
    actions = available_actions(state)
    if np.random.rand() < epsilon:
        return np.random.choice(actions)   # exploration
    else:
        q_vals = Q[state, actions]
        return actions[np.argmax(q_vals)]  # exploitation

def update(state, action):
    """Updated Q-learning rule with α learning rate."""
    max_future = np.max(Q[action])
    Q[state, action] = (1 - alpha) * Q[state, action] + \
                       alpha * (M[state, action] + gamma * max_future)


# -----------------------------
# TRAINING LOOP
# -----------------------------
scores = []
for i in range(2000):     # increased iterations
    state = np.random.randint(0, MATRIX_SIZE)
    action = epsilon_greedy(state)
    update(state, action)
    scores.append(np.max(Q))  # track max-q progress

# -----------------------------
# TESTING: EXTRACT BEST PATH
# -----------------------------
state = 0
path = [state]

while state != goal:
    actions = available_actions(state)
    next_state = actions[np.argmax(Q[state, actions])]
    path.append(next_state)
    state = next_state

print("\nNEW Most efficient path:")
print(path)

pl.plot(scores)
pl.title("Reward Curve (Updated Q-Learning)")
pl.xlabel("Iterations")
pl.ylabel("Max Q-value")
pl.show()


# -----------------------------
# ENVIRONMENT (Police / Drugs)
# -----------------------------
police = [2,4,5]
drug_traces = [3,8,9]

env_police = np.zeros((MATRIX_SIZE, MATRIX_SIZE))
env_drugs = np.zeros((MATRIX_SIZE, MATRIX_SIZE))

def collect_environmental_data(action):
    found = []
    if action in police:
        found.append("police")
    if action in drug_traces:
        found.append("drug")
    return found

def update_with_environment(state, action):
    """Q-learning + environmental matrices."""
    max_future = np.max(Q[action])
    Q[state, action] = (1 - alpha) * Q[state, action] + \
                       alpha * (M[state, action] + gamma * max_future)

    env = collect_environmental_data(action)
    if "police" in env:
        env_police[state, action] += 1
    if "drug" in env:
        env_drugs[state, action] += 1


# Second training phase (with environment)
for i in range(2000):
    state = np.random.randint(0, MATRIX_SIZE)
    action = epsilon_greedy(state)
    update_with_environment(state, action)

print("\nPolice Encounter Matrix:")
print(env_police)

print("\nDrug Trace Matrix:")
print(env_drugs)


# environmental-aware exploration
def available_actions_with_env(state):
    actions = available_actions(state)
    bad_actions = env_police[state, actions] > 0
    if np.any(~bad_actions):
        return actions[~bad_actions]
    return actions


# Third training using environment filtering
scores2 = []
for i in range(2000):
    state = np.random.randint(0, MATRIX_SIZE)
    actions = available_actions_with_env(state)
    action = sample_next_action(actions)
    update_with_environment(state, action)
    scores2.append(np.max(Q))

pl.plot(scores2)
pl.title("Reward Curve with Environmental Awareness")
pl.xlabel("Iterations")
pl.ylabel("Max Q-value")
pl.show()




#program3-RNN text generation#
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# -------------------------
# TEXT INPUT
# -------------------------
text = "The beautiful girl whom I met last time is very intelligent also"

chars = sorted(list(set(text)))
char_to_index = {char: i for i, char in enumerate(chars)}
index_to_char = {i: char for i, char in enumerate(chars)}

seq_length = 5
sequences = []
labels = []

for i in range(len(text) - seq_length):
    seq = text[i:i + seq_length]
    label = text[i + seq_length]
    sequences.append([char_to_index[c] for c in seq])
    labels.append(char_to_index[label])

X = np.array(sequences)
y = np.array(labels)

vocab_size = len(chars)

# -------------------------
# MODEL (Updated)
# -------------------------
model = Sequential()
model.add(Embedding(vocab_size, 32, input_length=seq_length))
model.add(LSTM(64, return_sequences=False))     # changed from SimpleRNN → LSTM
model.add(Dense(vocab_size, activation="softmax"))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
model.summary()

# -------------------------
# TRAINING
# -------------------------
model.fit(X, y, epochs=200, verbose=0)

# -------------------------
# TEMPERATURE SAMPLING FUNCTION
# -------------------------
def sample_with_temperature(preds, temp=0.7):
    preds = np.asarray(preds).astype("float64")
    preds = np.log(preds + 1e-8) / temp
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    return np.random.choice(len(preds), p=preds)

# -------------------------
# TEXT GENERATION
# -------------------------
start_seq = "The handsome boy whom I met "
generated_text = start_seq

text_len = 60
temperature = 0.6   # lower = more predictable, higher = more random

for _ in range(text_len):
    last_chars = generated_text[-seq_length:]
    x = np.array([[char_to_index[c] for c in last_chars]])

    preds = model.predict(x, verbose=0)[0]
    next_index = sample_with_temperature(preds, temp=temperature)
    next_char = index_to_char[next_index]

    generated_text += next_char

print("\nGenerated Text:")
print(generated_text)



#program4- Tic-Tac-Toe#
import numpy as np
import pickle

BOARD_ROWS = 3
BOARD_COLS = 3


class State:
    def __init__(self, p1, p2):
        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))
        self.p1 = p1
        self.p2 = p2
        self.isEnd = False
        self.boardHash = None
        self.playerSymbol = 1  # FIXED: always start with player 1

    def getHash(self):
        self.boardHash = str(self.board.reshape(BOARD_ROWS * BOARD_COLS))
        return self.boardHash

    def availablePositions(self):
        return [(i, j) for i in range(BOARD_ROWS)
                for j in range(BOARD_COLS) if self.board[i, j] == 0]

    def updateState(self, position):
        self.board[position] = self.playerSymbol
        self.playerSymbol *= -1

    def winner(self):
        for i in range(BOARD_ROWS):
            if abs(sum(self.board[i, :])) == 3:
                self.isEnd = True
                return np.sign(sum(self.board[i, :]))

        for i in range(BOARD_COLS):
            if abs(sum(self.board[:, i])) == 3:
                self.isEnd = True
                return np.sign(sum(self.board[:, i]))

        diag1 = sum(self.board[i, i] for i in range(BOARD_ROWS))
        diag2 = sum(self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_ROWS))
        if abs(diag1) == 3 or abs(diag2) == 3:
            self.isEnd = True
            return np.sign(diag1 if abs(diag1) == 3 else diag2)

        if not self.availablePositions():
            self.isEnd = True
            return 0

        self.isEnd = False
        return None

    def giveReward(self):
        result = self.winner()
        if result == 1:
            self.p1.feedReward(1)
            self.p2.feedReward(-1)
        elif result == -1:
            self.p1.feedReward(-1)
            self.p2.feedReward(1)
        else:
            self.p1.feedReward(0.2)
            self.p2.feedReward(0.2)

    def reset(self):
        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))
        self.boardHash = None
        self.isEnd = False
        self.playerSymbol = 1

    def play(self, rounds=20000):
        for _ in range(rounds):
            while not self.isEnd:
                positions = self.availablePositions()
                action = self.p1.chooseAction(positions, self.board, self.playerSymbol)
                self.updateState(action)
                self.p1.addState(self.getHash())

                win = self.winner()
                if win is not None:
                    self.giveReward()
                    self.p1.reset()
                    self.p2.reset()
                    self.reset()
                    break

                positions = self.availablePositions()
                action = self.p2.chooseAction(positions, self.board, self.playerSymbol)
                self.updateState(action)
                self.p2.addState(self.getHash())

                win = self.winner()
                if win is not None:
                    self.giveReward()
                    self.p1.reset()
                    self.p2.reset()
                    self.reset()
                    break

    def showBoard(self):
        symbols = {1: 'X', -1: 'O', 0: ' '}
        print("\n-------------")
        for i in range(BOARD_ROWS):
            print("|", end="")
            for j in range(BOARD_COLS):
                print(f" {symbols[self.board[i, j]]} |", end="")
            print("\n-------------")


class Player:
    def __init__(self, name, exp_rate=0.2):
        self.name = name
        self.lr = 0.3         # INCREASED learning rate
        self.exp_rate = exp_rate
        self.decay_gamma = 0.95
        self.states = []
        self.states_value = {}

    def getHash(self, board):
        return str(board.reshape(BOARD_ROWS * BOARD_COLS))

    def chooseAction(self, positions, board, symbol):
        if np.random.rand() < self.exp_rate:
            return positions[np.random.choice(len(positions))]

        value_max = -float('inf')
        action = None
        for p in positions:
            next_board = board.copy()
            next_board[p] = symbol
            hash_value = self.getHash(next_board)
            value = self.states_value.get(hash_value, 0)
            if value > value_max:
                value_max = value
                action = p
        return action

    def addState(self, state):
        self.states.append(state)

    def feedReward(self, reward):
        for st in reversed(self.states):
            self.states_value.setdefault(st, 0)
            self.states_value[st] += self.lr * (reward - self.states_value[st])
            reward *= self.decay_gamma

    def reset(self):
        self.states = []

    def savePolicy(self):
        with open('policy_' + self.name, 'wb') as f:
            pickle.dump(self.states_value, f)

    def loadPolicy(self, file):
        with open(file, 'rb') as f:
            self.states_value = pickle.load(f)


class HumanPlayer:
    def __init__(self, name):
        self.name = name

    def chooseAction(self, positions, *_):
        while True:
            row = int(input("Row (0-2): "))
            col = int(input("Col (0-2): "))
            if (row, col) in positions:
                return (row, col)


if __name__ == "__main__":
    p1 = Player("AI-1")
    p2 = Player("AI-2")

    print("Training AI...")
    st = State(p1, p2)
    st.play(30000)

    p1.savePolicy()

    p1 = Player("Computer", exp_rate=0)
    p1.loadPolicy("policy_AI-1")
    p2 = HumanPlayer("Human")

    st = State(p1, p2)

    while True:
        st.play2 = lambda: None  # reuse logic
        st.reset()
        while not st.isEnd:
            st.showBoard()
            action = p1.chooseAction(st.availablePositions(), st.board, st.playerSymbol)
            st.updateState(action)
            if st.winner() is not None:
                break
            st.showBoard()
            action = p2.chooseAction(st.availablePositions())
            st.updateState(action)
            if st.winner() is not None:
                break
        st.showBoard()
        print("Game Over!")
        if input("Play again? (y/n): ") != 'y':
            break




#program5- LSTM#
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

# -----------------------------
# LOAD DATA
# -----------------------------
data = pd.read_csv(
    '/content/international-airline-passengers.csv',
    skipfooter=5,
    engine='python'
)

dataset = data.iloc[:, 1].values.astype("float32")
plt.plot(dataset)
plt.xlabel("Time")
plt.ylabel("Number of Passengers")
plt.title("International Airline Passengers")
plt.show()

dataset = dataset.reshape(-1, 1)

# -----------------------------
# SCALING
# -----------------------------
scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(dataset)

# -----------------------------
# TRAIN-TEST SPLIT
# -----------------------------
train_size = int(len(dataset) * 0.8)   # changed from 75% → 80%
train, test = dataset[:train_size], dataset[train_size:]

# -----------------------------
# CREATE DATASET FUNCTION (NEW)
# -----------------------------
def create_dataset(data, time_step=12):
    X, Y = [], []
    for i in range(len(data) - time_step - 1):
        X.append(data[i:(i + time_step), 0])
        Y.append(data[i + time_step, 0])
    return np.array(X), np.array(Y)

time_stamp = 12   # changed from 10 → 12 (seasonality)
trainX, trainY = create_dataset(train, time_stamp)
testX, testY = create_dataset(test, time_stamp)

# reshape to [samples, time steps, features]
trainX = trainX.reshape(trainX.shape[0], time_stamp, 1)
testX = testX.reshape(testX.shape[0], time_stamp, 1)

# -----------------------------
# MODEL (UPDATED)
# -----------------------------
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(time_stamp, 1)))
model.add(Dropout(0.2))                      # NEW
model.add(LSTM(50))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')
model.summary()

# -----------------------------
# TRAINING
# -----------------------------
model.fit(trainX, trainY, epochs=100, batch_size=8, verbose=1)

# -----------------------------
# PREDICTIONS
# -----------------------------
trainPredict = model.predict(trainX)
testPredict = model.predict(testX)

# inverse scaling
trainPredict = scaler.inverse_transform(trainPredict)
trainY_inv = scaler.inverse_transform(trainY.reshape(-1, 1))
testPredict = scaler.inverse_transform(testPredict)
testY_inv = scaler.inverse_transform(testY.reshape(-1, 1))

# -----------------------------
# EVALUATION
# -----------------------------
trainScore = math.sqrt(mean_squared_error(trainY_inv, trainPredict))
testScore = math.sqrt(mean_squared_error(testY_inv, testPredict))
print(f'Train RMSE: {trainScore:.2f}')
print(f'Test RMSE: {testScore:.2f}')

# -----------------------------
# PLOTTING
# -----------------------------
trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:] = np.nan
trainPredictPlot[time_stamp:len(trainPredict)+time_stamp] = trainPredict

testPredictPlot = np.empty_like(dataset)
testPredictPlot[:] = np.nan
testPredictPlot[len(trainPredict)+(time_stamp*2)+1:len(dataset)-1] = testPredict

plt.plot(scaler.inverse_transform(dataset), label="Actual Data")
plt.plot(trainPredictPlot, label="Train Prediction")
plt.plot(testPredictPlot, label="Test Prediction")
plt.legend()
plt.title("LSTM Passenger Forecast (Updated Model)")
plt.show()
